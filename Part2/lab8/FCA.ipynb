{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a OL using FCA:\n",
    "\n",
    "- Python Library for FCA\n",
    "\n",
    "  [https://pypi.org/project/concepts/](https://pypi.org/project/concepts/)\n",
    "\n",
    "- Your choice (both the \"concepts\" and the features).\n",
    "  - Example features: semantic properties, related words, synsets, occurrence in documents, etc.\n",
    "  \n",
    "Example (useful but not limited to):\n",
    "- Given two input languages (e.g., Italian and English):\n",
    "  - Concepts: terms (from both languages).\n",
    "  - Features: membership synsets.\n",
    "\n",
    "### Functions\n",
    "\n",
    "In our implementation we use 2 different type of data: the first one type are 2 list of word (english and italian), the second one is a topic took from wikipedia using the api.\n",
    "#### `get_info(syns)`\n",
    "\n",
    "This function takes a list of synsets (WordNet word senses) as input and returns a list of strings containing information about their parts of speech and names. It does the following:\n",
    "\n",
    "1. Initializes an empty set `pos_set` to keep track of unique parts of speech.\n",
    "2. Initializes an empty list `info` to store information about the synsets.\n",
    "3. Iterates through the synsets:\n",
    "   - Retrieves the part of speech using `syn.pos()`.\n",
    "   - Checks if the part of speech has been encountered before. If yes, it skips to the next iteration.\n",
    "   - Adds the part of speech to `pos_set` to track unique occurrences.\n",
    "   - Based on the part of speech, appends a corresponding string to the `info` list ('noun' for 'n', 'adjective' for 'a', 'verb' for 'v').\n",
    "   - If there are synsets (not an empty list), appends the name of the first synset to the `info` list.\n",
    "4. Returns the `info` list containing part of speech and synset name information.\n",
    "\n",
    "\n",
    "### get_data \n",
    "The function get_data  sets up a user agent and uses the WikipediaAPI to retrieve summaries for specific Wikipedia pages, which are defined in the 'titles' list. These summaries are then joined together and returned as a single string.\n",
    "\n",
    "\n",
    "### extract_concepts\n",
    "The extract_concepts function processes the content by tokenizing it into words, removing stopwords and non-alphabetic tokens, and extracting nouns. These nouns are considered as concepts.\n",
    "\n",
    "### Creating Definitions using Synsets\n",
    "\n",
    "The code block then creates definitions for the generated words using WordNet synsets.\n",
    "\n",
    "1. Initializes a `Definition` object named `d` to store word definitions.\n",
    "2. Iterates through each word in the combined list of English and Italian.\n",
    "   - Determines the language of the word ('eng' for English and 'ita' for Italian).\n",
    "   - Retrieves the synsets of the word from WordNet using `wn.synsets()`.\n",
    "   - Calls the `get_info()` function to extract information about the synsets (part of speech and name).\n",
    "   - Adds the word and its associated information to the `d` object using `d.add_object()`.\n",
    "3. Creates a `Context` named `c` using the populated `Definition` object `d`.\n",
    "\n",
    "### Visualizing the Lattice and Printing the Context\n",
    "\n",
    "The code finishes by visualizing the lattice of the context and printing the context itself.\n",
    "\n",
    "1. Calls the `graphviz()` function on the lattice of the `Context` object `c` to visualize it using Graphviz (if available).\n",
    "2. Prints the `Context` object `c`, which includes the word definitions and relationships based on the WordNet synsets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/palius/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/palius/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/palius/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from concepts import Context, Definition\n",
    "import wikipediaapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_info(syns) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns the information about the parts of speech (POS) of the given synonyms.\n",
    "    :param syns: A list of synonyms\n",
    "    :return: A list of POS information for each synonym\n",
    "    \"\"\"\n",
    "    pos_set = set()\n",
    "    info = []\n",
    "    for syn in syns:\n",
    "        pos = syn.pos()\n",
    "        if pos in pos_set:\n",
    "            continue\n",
    "        pos_set.add(pos)\n",
    "        if pos == 'n':\n",
    "            info.append('noun')\n",
    "        elif pos == 'a':\n",
    "            info.append('adjective')\n",
    "        elif pos == 'v':\n",
    "            info.append('verb')\n",
    "    \n",
    "    if syns:  # Check if syns list is not empty\n",
    "        info.append(syns[0].name())\n",
    "    \n",
    "    return info\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"\n",
    "    Retrieves data from Wikipedia based on the provided titles.\n",
    "    :return: A string containing the summaries of the Wikipedia pages\n",
    "    \"\"\"\n",
    "    user_agent = \"university_project/1.0\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent, 'en')\n",
    "\n",
    "    titles = [\n",
    "        'Mariano Rivera'\n",
    "    ]\n",
    "    summaries = [wiki_wiki.page(title).summary for title in titles]\n",
    "\n",
    "    return ' '.join(summaries)\n",
    "\n",
    "def extract_concepts(content):\n",
    "    \"\"\"\n",
    "    Extracts noun keywords from the given content.\n",
    "    :param content: The content to extract keywords from\n",
    "    :return: A list of noun keywords\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(content)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word.lower().isalpha()]\n",
    "    tagged_tokens = pos_tag(filtered_tokens)\n",
    "    noun_keywords = [word for word, pos in tagged_tokens if pos.startswith('N')]   #concepts are nouns\n",
    "    return noun_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERSION WITH ENGLISH AND ITALIAN WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Context object mapping 10 objects to 7 properties [82009819] at 0x7f527ebfffd0>\n",
      "           |noun|verb|dog.n.01|cat.n.01|car.n.01|door.n.01|bird.n.01|\n",
      "    dog    |X   |X   |X       |        |        |         |         |\n",
      "    cat    |X   |X   |        |X       |        |         |         |\n",
      "    car    |X   |    |        |        |X       |         |         |\n",
      "    door   |X   |    |        |        |        |X        |         |\n",
      "    bird   |X   |X   |        |        |        |         |X        |\n",
      "    cane   |X   |    |X       |        |        |         |         |\n",
      "    gatto  |X   |    |        |X       |        |         |         |\n",
      "    auto   |X   |    |        |        |X       |         |         |\n",
      "    porta  |X   |    |        |        |        |X        |         |\n",
      "    uccello|X   |    |        |        |        |         |X        |\n"
     ]
    }
   ],
   "source": [
    "# English words\n",
    "en_words = [\"dog\", \"cat\", \"car\", \"door\", \"bird\"]\n",
    "\n",
    "# Italian translations\n",
    "it_words = [\"cane\", \"gatto\", \"auto\", \"porta\", \"uccello\"]\n",
    "\n",
    "# Create a Definition object\n",
    "d = Definition()\n",
    "\n",
    "# Iterate over both English and Italian words\n",
    "for word in en_words + it_words:\n",
    "    # Determine the language of the word\n",
    "    lang = 'eng' if word in en_words else 'ita'\n",
    "    \n",
    "    # Get the synsets (meaning) of the word in the specified language\n",
    "    syns = wn.synsets(word, lang=lang)\n",
    "    \n",
    "    # Get the information about the parts of speech (POS) of the synsets\n",
    "    w_info = get_info(syns)\n",
    "    \n",
    "    # Add the word and its information to the Definition object\n",
    "    d.add_object(word, w_info)\n",
    "\n",
    "# Create a Context object using the Definition object\n",
    "c = Context(*d)\n",
    "\n",
    "# Generate a graph visualization of the lattice and display it\n",
    "c.lattice.graphviz(view=True)\n",
    "\n",
    "# Print the Context object\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERSION WITH DATA RETRIVED WITH WIKIPEDIA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Context object mapping 9 objects to 10 properties [65422e3e] at 0x7f527b2d7160>\n",
      "            |noun|rivera.n.01|baseball.n.01|pitcher.n.01|verb|season.n.01|league.n.01|york.n.01|yankee.n.01|career.n.01|\n",
      "    mariano |    |           |             |            |    |           |           |         |           |           |\n",
      "    rivera  |X   |X          |             |            |    |           |           |         |           |           |\n",
      "    baseball|X   |           |X            |            |    |           |           |         |           |           |\n",
      "    pitcher |X   |           |             |X           |    |           |           |         |           |           |\n",
      "    seasons |X   |           |             |            |X   |X          |           |         |           |           |\n",
      "    league  |X   |           |             |            |X   |           |X          |         |           |           |\n",
      "    york    |X   |           |             |            |    |           |           |X        |           |           |\n",
      "    yankees |X   |           |             |            |    |           |           |         |X          |           |\n",
      "    career  |X   |           |             |            |X   |           |           |         |           |X          |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/palius/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "\n",
      "(process:70722): Gtk-WARNING **: 15:58:29.008: Locale not supported by C library.\n",
      "\tUsing the fallback 'C' locale.\n",
      "/home/palius/snap/code/common/.cache/gio-modules/libgiolibproxy.so: cannot open shared object file: Permission denied\n",
      "Failed to load module: /home/palius/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "\n",
      "(evince:70722): Gtk-WARNING **: 15:58:29.078: Theme parsing error: gtk-keys.css:1:0: Failed to import: Error opening file /snap/code/138/usr/share/themes/Default/gtk-3.0/gtk-keys.css: Permission denied\n",
      "Gtk-Message: 15:58:29.078: Failed to load module \"canberra-gtk-module\"\n",
      "Gtk-Message: 15:58:29.079: Failed to load module \"canberra-gtk-module\"\n",
      "/home/palius/snap/code/common/.cache/gio-modules/libdconfsettings.so: cannot open shared object file: Permission denied\n",
      "Failed to load module: /home/palius/snap/code/common/.cache/gio-modules/libdconfsettings.so\n",
      "\n",
      "(evince:70722): Gtk-WARNING **: 15:58:29.352: Could not load a pixbuf from icon theme.\n",
      "This may indicate that pixbuf loaders or the mime database could not be found.\n"
     ]
    }
   ],
   "source": [
    "words=extract_concepts(get_data())\n",
    "\n",
    "d = Definition()\n",
    "for word in words[:10]:\n",
    "    syns = wn.synsets(word)\n",
    "    w_info = get_info(syns)\n",
    "    d.add_object(word, w_info)\n",
    "\n",
    "\n",
    "c = Context(*d)\n",
    "c.lattice.graphviz(view=True)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
