{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to generate the dataset and reprocess it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wikipediaapi\n",
    "import re\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(topics):\n",
    "    \"\"\"\n",
    "    Fetches Wikipedia text content for a list of topics and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "        topics (list): A list of topic titles for fetching content.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    user_agent = \"university_project/1.0\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent, 'en')\n",
    "    \n",
    "    # Fetch content for topics and join them into a single corpus\n",
    "    corpus = ' '.join([wiki_wiki.page(topic).text for topic in topics]) \n",
    "    \n",
    "    # Replace multiple newlines with a single newline\n",
    "    corpus = re.sub('\\n+', '\\n', corpus)\n",
    "    \n",
    "    # Write the corpus to a file\n",
    "    with open('res/corpus.txt', \"w\") as file:\n",
    "        file.write(corpus)\n",
    "\n",
    "\n",
    "def load_topics(file_path):\n",
    "    \"\"\"\n",
    "    Loads topics from a file and returns them as a list.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing topics.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of topics.\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            topics.append(line.strip())\n",
    "    return topics\n",
    "\n",
    "topics = load_topics('res/guitar_players.txt')\n",
    "\n",
    "get_corpus(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads sentences from a file and returns them as a list.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing sentences.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                sentence = line.strip()\n",
    "                sentences.append(sentence)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "verb = 'play'\n",
    "\n",
    "def filter_corpus(corpus, verb):\n",
    "    \"\"\"\n",
    "    Filters relevant sentences from the corpus based on a specific verb.\n",
    "\n",
    "    Args:\n",
    "        corpus (list): A list of sentences to filter.\n",
    "        verb (str): The verb to search for in the sentences.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of relevant sentences containing the specified verb.\n",
    "    \"\"\"\n",
    "    relevant_sentences = []\n",
    "    for text in corpus:\n",
    "        processed_text = nlp(text)\n",
    "        for sentence in processed_text.sents:\n",
    "            for token in sentence:\n",
    "                if token.lemma_ == verb:\n",
    "                    relevant_sentences.append(sentence.text)\n",
    "                    break\n",
    "\n",
    "    with open(\"../lab3/res/filtered_corpus.txt\", \"w\") as file:\n",
    "        for sentence in relevant_sentences:\n",
    "            file.write(sentence + \"\\n\")\n",
    "    return relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_data('../lab3/res/corpus.txt')\n",
    "filtered_corpus = filter_corpus(corpus, verb)\n",
    "sentences = load_data('../lab3/res/filtered_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text data by removing stopwords and punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = string.punctuation\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words and not w in punctuation]\n",
    "\n",
    "    clean_text = ' '.join(filtered_text)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def substitute(sentences, names):\n",
    "    \"\"\"\n",
    "    Substitutes names in sentences with 'person' and cleans the sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of sentences to process.\n",
    "        names (list): List of names to be substituted.\n",
    "\n",
    "    Returns:\n",
    "        list: List of sentences with substituted names and cleaned text.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(name) for name in names) + r')\\b'\n",
    "\n",
    "    replaced_sentences = []\n",
    "    for sentence in sentences:\n",
    "        replaced_sentence = re.sub(pattern, 'person', sentence, flags=re.IGNORECASE)\n",
    "        replaced_sentence = re.sub(r'\\b(it)\\b', 'thing', replaced_sentence, flags=re.IGNORECASE)\n",
    "        replaced_sentence = clean_text(replaced_sentence)\n",
    "        replaced_sentences.append(replaced_sentence)\n",
    "\n",
    "    with open(\"../lab3/res/sentences.txt\", \"w\") as file:\n",
    "        for sentence in replaced_sentences:\n",
    "            file.write(sentence + \"\\n\")\n",
    "\n",
    "    return replaced_sentences\n",
    "\n",
    "def load_txt(file_path):\n",
    "    \"\"\"\n",
    "    Loads text data from a file and returns it as a list of rows.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file containing text data.\n",
    "\n",
    "    Returns:\n",
    "        list: List of rows from the file.\n",
    "    \"\"\"\n",
    "    row = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            row.append(line.strip())\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = load_txt('../lab3/res/names.txt')\n",
    "pronouns = load_txt('../lab3/res/pronouns.txt')\n",
    "n_and_p = names + pronouns\n",
    "filtered_sentences = substitute(sentences, n_and_p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
