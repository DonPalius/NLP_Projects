{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to simulate the game of **Ghigliottina**. In this game, the player is given 5 words and must identify a sixth word that is semantically related to the initial 5. To achieve this, we have implemented two distinct strategies:\n",
    "\n",
    "1. **Similarity-based Approach**: This strategy involves using similarity measures to determine the related word. We calculate the similarity between the given words and potential candidates to find the most closely related one.\n",
    "\n",
    "2. **Frequency-based Approach**: In contrast, the second strategy relies on the frequency of synsets. We analyze the frequency of WordNet synsets associated with the given words and select the most frequent synset as the related word.\n",
    "\n",
    "These strategies provide different ways to approach the problem and showcase different aspects of word semantics. The goal is to compare their effectiveness in finding a semantically related word in the game of Ghigliottina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_csv_to_dict(csv_filename):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file into a dictionary.\n",
    "\n",
    "    Args:\n",
    "        csv_filename (str): The name of the CSV file to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are words (converted to lowercase) and values\n",
    "              are lists of sentences associated with each word.\n",
    "    \"\"\"\n",
    "    word_dict = {}\n",
    "    \n",
    "    with open(csv_filename, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)\n",
    "        \n",
    "        for row in csvreader:\n",
    "            if len(row) >= 2:\n",
    "                word = row[0].lower()\n",
    "                sentences = row[1].split('. ')\n",
    "                word_dict[word] = sentences\n",
    "    \n",
    "    return word_dict\n",
    "\n",
    "\n",
    "\n",
    "csv_filename = './res/data.csv'  \n",
    "word_dict = load_csv_to_dict(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_column(csv_file):\n",
    "    \"\"\"\n",
    "    Read the first column of a CSV file and return it as a list of lowercase strings.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): The name of the CSV file to be read.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lowercase strings representing the values in the first column of the CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file, usecols=[0])\n",
    "    first_column = df.iloc[:, 0].str.lower().tolist()\n",
    "    return first_column\n",
    "\n",
    "csv_file = './res/data.csv'\n",
    "first_column_list = read_first_column(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_senses_for_word(word, word_sentences_dict):\n",
    "    \"\"\"\n",
    "    Disambiguate senses of a word within context sentences.\n",
    "\n",
    "    This function takes a target 'word' and a dictionary 'word_sentences_dict'\n",
    "    containing sentences associated with words. It attempts to disambiguate the\n",
    "    senses of the target word within its context sentences using Lesk algorithm.\n",
    "\n",
    "    Args:\n",
    "        word (str): The target word to disambiguate.\n",
    "        word_sentences_dict (dict): A dictionary where keys are words and values\n",
    "            are lists of sentences associated with each word.\n",
    "\n",
    "    Returns:\n",
    "        set: A set containing the disambiguated senses (WordNet synsets) of the 'word'\n",
    "            based on its context sentences. If no senses are found, an empty set is returned.\n",
    "    \"\"\"\n",
    "    disambiguated_senses = set()\n",
    "    \n",
    "    if word in word_sentences_dict:\n",
    "        sentences = word_sentences_dict[word]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            synsets = wordnet.synsets(word)\n",
    "            \n",
    "            if synsets:\n",
    "                best_sense = lesk(sentence.split(), word, synsets=synsets)\n",
    "                disambiguated_senses.add(best_sense)\n",
    "    \n",
    "    return disambiguated_senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_synsets(synset):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve related synsets for a given synset.\n",
    "\n",
    "    This function uses the hypernyms, hyponyms, part meronyms, and substance meronyms\n",
    "    relations to find related synsets.\n",
    "\n",
    "    Args:\n",
    "    synset: nltk.corpus.reader.wordnet.Synset\n",
    "        The synset for which related synsets are to be found.\n",
    "\n",
    "    Returns:\n",
    "    list: \n",
    "        A list of nltk.corpus.reader.wordnet.Synset objects that are related to the input synset.\n",
    "    \"\"\"\n",
    "     \n",
    "    related_synsets = []\n",
    "    \n",
    "    hypernyms = synset.hypernyms()\n",
    "    for hypernym in hypernyms:\n",
    "        related_synsets.append(hypernym)\n",
    "            \n",
    "    hyponyms = synset.hyponyms()\n",
    "    for hyponym in hyponyms:\n",
    "        related_synsets.append(hyponym)\n",
    "\n",
    "    part_meronyms= synset.part_meronyms()\n",
    "    for meronym in part_meronyms:\n",
    "        related_synsets.append(meronym)\n",
    "\n",
    "    substance_meronyms= synset.substance_meronyms()\n",
    "    for meronym in substance_meronyms:\n",
    "        related_synsets.append(meronym)\n",
    "            \n",
    "    return related_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_related_synsets(word_list, word_sentences_dict):\n",
    "    \"\"\"\n",
    "    Retrieve related WordNet synsets for a list of words within context sentences.\n",
    "\n",
    "    This function takes a list of target 'word_list' and a dictionary 'word_sentences_dict'\n",
    "    containing sentences associated with words. It retrieves related WordNet synsets for\n",
    "    the target words based on their context sentences and disambiguated senses.\n",
    "\n",
    "    Args:\n",
    "        word_list (list): A list of words for which related synsets need to be retrieved.\n",
    "        word_sentences_dict (dict): A dictionary where keys are words and values\n",
    "            are lists of sentences associated with each word.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing related WordNet synsets for the input 'word_list' based on\n",
    "            their context sentences and disambiguated senses.\n",
    "    \"\"\"\n",
    "    related_synsets = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        disambiguated_senses = disambiguate_senses_for_word(word, word_sentences_dict)\n",
    "        for sense in disambiguated_senses:\n",
    "            related_synsets.extend(get_related_synsets(sense))\n",
    "    \n",
    "    return related_synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when working with similarities the results are not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_synset(synsets):\n",
    "    \"\"\"\n",
    "    Find the closest WordNet synset from a list of synsets based on similarity measures.\n",
    "\n",
    "    This function takes a list of WordNet synsets 'synsets' and calculates the average similarity\n",
    "    between each synset and the other synsets in the list. It returns the synset with the highest\n",
    "    average similarity as the closest synset.\n",
    "\n",
    "    Args:\n",
    "        synsets (list): A list of WordNet synsets for which the closest synset needs to be found.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The name of the closest WordNet synset (without sense number), or None if\n",
    "            no synsets are provided or if the calculation cannot be performed.\n",
    "    \"\"\"\n",
    "    closest_synset = None\n",
    "    max_avg_similarity = -1\n",
    "    \n",
    "    for synset in synsets:\n",
    "        total_similarity = 0\n",
    "        count = 0\n",
    "        \n",
    "        for other_synset in synsets:\n",
    "            if synset != other_synset:\n",
    "                if synset.pos() == other_synset.pos():  # Check if parts of speech match\n",
    "                    wup_similarity = synset.wup_similarity(other_synset)\n",
    "                    shortest_path = synset.shortest_path_distance(other_synset)\n",
    "                    lch_similarity = synset.lch_similarity(other_synset)\n",
    "                    \n",
    "                    if wup_similarity is not None and shortest_path is not None and lch_similarity is not None:\n",
    "                        total_similarity += (wup_similarity + (1 / (1 + shortest_path)) + lch_similarity)\n",
    "                        count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            avg_similarity = total_similarity / count\n",
    "            \n",
    "            if avg_similarity > max_avg_similarity:\n",
    "                max_avg_similarity = avg_similarity\n",
    "                closest_synset = synset\n",
    "    \n",
    "    if closest_synset:\n",
    "        return closest_synset.name().split('.')[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working with frequency yields better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_most_frequent_synset(synsets, five_words):\n",
    "    \"\"\"\n",
    "    Find the most frequent synset among a list of synsets, excluding certain words.\n",
    "\n",
    "    This function counts the frequency of each synset in the input list, ignoring synsets\n",
    "    that match any of the words in the five_words list. It then returns the most frequent synset.\n",
    "\n",
    "    Parameters:\n",
    "    synsets : \n",
    "        The list of synsets to analyze.\n",
    "    five_words : \n",
    "        The list of words to exclude from the frequency count.\n",
    "\n",
    "    Returns:\n",
    "    str:\n",
    "        The name of the most frequent synset in the input list, or None if there are no valid synsets.\n",
    "    \"\"\"\n",
    "\n",
    "    synset_counts = {}\n",
    "    \n",
    "    for synset in synsets:\n",
    "        synset_name = synset.name().split('.')[0]\n",
    "        if synset_name not in five_words:\n",
    "            if synset_name in synset_counts:\n",
    "                synset_counts[synset_name] += 1\n",
    "            else:\n",
    "                synset_counts[synset_name] = 1\n",
    "    \n",
    "    most_frequent_synset = None\n",
    "    max_count = -1\n",
    "    \n",
    "    for synset_name, count in synset_counts.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            most_frequent_synset = synset_name\n",
    "    \n",
    "    return most_frequent_synset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's how we simulate the game:\n",
    "\n",
    "1. Randomly selects 5 words from the `first_column_list`.\n",
    "\n",
    "2. Retrieves related WordNet synsets for the 5 selected words using the `retrieve_related_synsets` function.\n",
    "\n",
    "3. Finds the closest WordNet synset using the similarity approach with `find_closest_synset` function.\n",
    "\n",
    "4. Finds the most frequent synset for the selected words using the `find_most_frequent_synset` function.\n",
    "\n",
    "5. Prints the result of the similarity-based approach, which is the closest synset.\n",
    "\n",
    "6. Prints the result of the frequency-based approach, which is the most frequent synset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'pitch')\n",
      "(2, 'time')\n",
      "(3, 'interest')\n",
      "(4, 'spring')\n",
      "(5, 'shop')\n",
      "Similarity based approach: jump\n",
      "Frequency based approach: alto\n"
     ]
    }
   ],
   "source": [
    "five_words = random.sample(first_column_list, 5)\n",
    "rel_syns = retrieve_related_synsets(five_words, word_dict)\n",
    "\n",
    "closest_synset = find_closest_synset(rel_syns)\n",
    "most_freq_synset = find_most_frequent_synset(rel_syns, five_words)\n",
    "\n",
    "for word in enumerate (five_words, start = 1):\n",
    "    print(word)\n",
    "print(f'Similarity based approach: {closest_synset}')\n",
    "print(f'Frequency based approach: {most_freq_synset}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
