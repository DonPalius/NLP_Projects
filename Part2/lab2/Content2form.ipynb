{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5316520e",
   "metadata": {},
   "source": [
    "## Code Explanation: Content-to-Form Experiment using WordNet Definitions\n",
    "\n",
    "The code implements a content-to-form experiment utilizing WordNet definitions. The experiment focuses on disambiguating word senses for various concepts, such as \"door,\" \"ladybug,\" \"pain,\" and \"blurriness.\" For each concept, the code takes available definitions, searches for the correct synset in WordNet, and employs the principle of \"genus\" to guide the search.\n",
    "\n",
    "The primary goal of this experiment is to showcase the process of disambiguating word senses using the modified Lesk algorithm. By leveraging the provided definitions and utilizing the structure of WordNet, the code aims to identify the most appropriate sense for each word within its specific context.\n",
    "\n",
    "### Import and Load Data\n",
    "\n",
    "The code begins by importing necessary libraries and loading definitions from a TSV file. The definitions are stored in a dictionary named `definitions`, where the keys are words, and the values are lists of their corresponding definitions.\n",
    "\n",
    "### Word Preprocessing\n",
    "\n",
    "Before performing word disambiguation, the code defines several utility functions for word preprocessing:\n",
    "\n",
    "1. `remove_punctuation(sentence)`: This function removes punctuation characters from a sentence except for hyphens.\n",
    "2. `clean_and_split(text, stopwords)`: This function cleans and processes text by removing punctuation, splitting into words, converting to lowercase, lemmatizing, and excluding stopwords.\n",
    "3. `get_words_frequency(words)`: This function calculates the frequency of words.\n",
    "4. `concat(l1, l2)`: This function concatenates two lists.\n",
    "5. `get_hyponyms(word)`: This function retrieves the hyponyms (more specific terms) of a given word using WordNet from NLTK.\n",
    "6. `signature(sense, stop_words)`: This function computes the signature of a WordNet synset, including its name and example words.\n",
    "7. `computeOverlap(signature, context)`: This function calculates the overlap between two sets of words.\n",
    "8. `modifiedLesk(words, disambiguation_context, stop_words)`: This function implements a modified version of the Lesk algorithm to find probable synsets for candidate genera.\n",
    "\n",
    "### Word Disambiguation\n",
    "\n",
    "For each of the words - \"door,\" \"ladybug,\" \"pain,\" and \"blurriness,\" the code performs the following steps:\n",
    "\n",
    "1. Clean and preprocess each definition, storing the lemmatized words in the `clean_words` list.\n",
    "2. Combine all cleaned word sets to create a disambiguation context.\n",
    "3. Calculate the frequency of words in the cleaned definitions.\n",
    "4. Select the top 15 most frequent words as candidate genera.\n",
    "5. Apply the modified Lesk algorithm to rank probable senses for the candidate genera, based on the disambiguation context.\n",
    "\n",
    "### Results\n",
    "\n",
    "For each word, the code prints the top 5 most probable senses along with their corresponding scores. This information helps to identify the most suitable sense of each word within its specific context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d5bf33bc859818",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## import and load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:52.871332665Z",
     "start_time": "2023-08-18T22:02:52.671574507Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import WordNetLemmatizer\n",
    "import string\n",
    "import functools as ft\n",
    "from nltk.corpus import wordnet as wn\n",
    "import heapq as hq\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"nltk.corpus.reader.wordnet\")\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "# Load definitions from the TSV file\n",
    "definitions = {}\n",
    "words = []\n",
    "\n",
    "with open(\"../lab2/TLN-definitions-23.tsv\", \"r\", encoding=\"utf-8\") as tsvfile:\n",
    "    reader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    \n",
    "    # Read the first row to get the words\n",
    "    words = next(reader)[1:]\n",
    "    \n",
    "    for row in reader:\n",
    "        for i, definition in enumerate(row[1:]):\n",
    "            word = words[i]\n",
    "            if word not in definitions:\n",
    "                definitions[word] = []\n",
    "            definitions[word].append(definition)\n",
    "\n",
    "keys = list(definitions.keys())\n",
    "\n",
    "door_def = definitions[keys[0]]\n",
    "ladybug_def= definitions[keys[1]]\n",
    "pain_def = definitions[keys[2]]\n",
    "blurriness_def = definitions[keys[3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c24c82e081506c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Word preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee78fbe9a60c20b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:52.883811778Z",
     "start_time": "2023-08-18T22:02:52.688224765Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, this function removes all punctuation characters except for the hyphen (-).\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (str): The sentence from which punctuation should be removed.\n",
    "\n",
    "    Returns:\n",
    "    str: The sentence with all punctuation except hyphens removed.\n",
    "    \"\"\"\n",
    "    for character in string.punctuation:\n",
    "        if character not in ['-']:\n",
    "            sentence = sentence.replace(character, '')\n",
    "    return sentence\n",
    "\n",
    "def clean_and_split(text, stopwords):\n",
    "    \"\"\"\n",
    "    Given a text, this function performs the following operations:\n",
    "    1. Removes punctuation except for hyphens using the remove_punctuation function.\n",
    "    2. Splits the text into individual words.\n",
    "    3. Converts each word to lowercase.\n",
    "    4. Lemmatizes each word using the WordNetLemmatizer.\n",
    "    5. Filters out any words that are in the provided list of stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The text to be cleaned and split.\n",
    "    stopwords (list): A list of words to be excluded from the final list.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lemmatized words from the text, excluding any stopwords.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = remove_punctuation(text)\n",
    "    return [lemmatizer.lemmatize(w.lower()) for w in text.split() if w.lower() not in stopwords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15834ef3c6191b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:52.903682615Z",
     "start_time": "2023-08-18T22:02:52.731596357Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words_frequency(words):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of the input words.\n",
    "    \n",
    "    Parameters:\n",
    "    words (list): The list of words for which to calculate frequencies.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples (word, frequency), sorted in descending order by frequency.\n",
    "    \"\"\"\n",
    "    count = {}\n",
    "    for word in words:\n",
    "        if word in count:\n",
    "            count[word] += 1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    words_and_counts = [(w, count[w]) for w in count]\n",
    "    words_and_counts.sort(key=lambda wac: wac[1], reverse=True)\n",
    "    return words_and_counts\n",
    "\n",
    "def concat(l1, l2):\n",
    "    \"\"\"\n",
    "    Concatenates two lists by extending the first list with the second one.\n",
    "    \n",
    "    Parameters:\n",
    "    l1 (list): The first list to which the second list will be added.\n",
    "    l2 (list): The second list which will be added to the first list.\n",
    "\n",
    "    Returns:\n",
    "    list: The concatenated list.\n",
    "    \"\"\"\n",
    "    l1.extend(l2)\n",
    "    return l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "965dcc02d26b1a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:52.904161280Z",
     "start_time": "2023-08-18T22:02:52.733139376Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hyponyms(word):\n",
    "    \"\"\"\n",
    "    Retrieves the hyponyms (more specific terms) and hypernyms (more general terms) \n",
    "    of a given word using the WordNet corpus from the NLTK library.\n",
    "    \n",
    "    Parameters:\n",
    "    word (str): The word for which to find the hyponyms and hypernyms.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of hyponyms and hypernyms for the given word.\n",
    "    \"\"\"\n",
    "    senses = wn.synsets(word)\n",
    "    hyponyms = [list(s.closure(lambda s: s.hyponyms())) for s in senses]\n",
    "    non_empty_hyponyms = [h for h in hyponyms if h]  # Filter out empty lists\n",
    "    if non_empty_hyponyms:\n",
    "        senses.extend(ft.reduce(concat, non_empty_hyponyms))\n",
    "\n",
    "    return senses\n",
    "\n",
    "\n",
    "def signature(sense, stop_words):\n",
    "    \"\"\"\n",
    "    Computes the signature of a WordNet synset, namely its name and the words contained in its examples.\n",
    "    \n",
    "    Parameters:\n",
    "    sense (str): The WordNet synset.\n",
    "    stop_words (list): A list of words to ignore.\n",
    "\n",
    "    Returns:\n",
    "    set: The set of words that compose the signature of the sense.\n",
    "    \"\"\"\n",
    "    s = sense.definition()\n",
    "    for e in sense.examples():\n",
    "        s = s + \" \" + e\n",
    "    return set(clean_and_split(s, stop_words))\n",
    "\n",
    "def computeOverlap(signature, context):\n",
    "    \"\"\"\n",
    "    Computes the size of the overlap between two sets of words.\n",
    "    \n",
    "    Parameters:\n",
    "    signature (set): The first set of words.\n",
    "    context (set): The second set of words.\n",
    "\n",
    "    Returns:\n",
    "    int: The size of the overlap between the two sets.\n",
    "    \"\"\"\n",
    "    return len(signature & context)\n",
    "\n",
    "import heapq as hq\n",
    "\n",
    "def modifiedLesk(words, disambiguation_context, stop_words):\n",
    "    \"\"\"\n",
    "    Modified version of the Lesk algorithm that returns a list of probable synsets for a set of candidate genera.\n",
    "    \n",
    "    Parameters:\n",
    "    words (list): The possible genera.\n",
    "    disambiguation_context (list): The disambiguation context to use.\n",
    "    stop_words (list): A list of words to ignore.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples (-score, synset, definition) organized in a minimum heap (the most probable has the lowest score).\n",
    "    \"\"\"\n",
    "    senses_rank = []\n",
    "    explored = {}\n",
    "    for word in words:\n",
    "        senses = get_hyponyms(word)\n",
    "        for sense in senses:\n",
    "            sign = signature(sense, stop_words)\n",
    "            context_set = set(disambiguation_context)\n",
    "            overlap = computeOverlap(sign, context_set)\n",
    "            if sense.name() not in explored:\n",
    "                explored[sense.name()] = overlap\n",
    "                # Include synset and definition in the tuple\n",
    "                synset_tuple = (-overlap, sense, sense.definition())\n",
    "                hq.heappush(senses_rank, synset_tuple)\n",
    "    return senses_rank\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a4b2bf59b349a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### door\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f7bdb6cc6ffaa20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:54.753587766Z",
     "start_time": "2023-08-18T22:02:52.733500998Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term we are looking for: door\n",
      "\n",
      "\n",
      "Score: 9\n",
      "Synset name: doorway.n.01\n",
      "Definition: the entrance (the space in a wall) through which you enter or leave a room or building; the space that a door can close\n",
      "\n",
      "Score: 7\n",
      "Synset name: leave.v.06\n",
      "Definition: make a possibility or provide opportunity for; permit to be attainable or cause to remain\n",
      "\n",
      "Score: 7\n",
      "Synset name: partition.n.01\n",
      "Definition: a vertical structure that divides or separates (as a wall divides one room from another)\n",
      "\n",
      "Score: 7\n",
      "Synset name: wall.n.01\n",
      "Definition: an architectural partition with a height and length greater than its thickness; used to divide or enclose an area or to support another structure\n",
      "\n",
      "Score: 6\n",
      "Synset name: adapter.n.02\n",
      "Definition: device that enables something to be used in a way different from that for which it was intended or makes different pieces of apparatus compatible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the cleaned words\n",
    "clean_words = []\n",
    "\n",
    "# For each definition in the list of door definitions\n",
    "for definition in door_def:\n",
    "    # Clean and split the definition into individual words, excluding any stopwords\n",
    "    # Add the list of cleaned words to the clean_words list\n",
    "    clean_words = [clean_and_split(definition, stop_words) for definition in door_def]\n",
    "\n",
    "# Reduce the list of sets of words (one set for each cleaned definition) to a single set containing all words\n",
    "# This set represents the disambiguation context\n",
    "disambiguation_context = ft.reduce(set.union, [set(words) for words in clean_words])\n",
    "\n",
    "\n",
    "# Calculate the frequency of each word in the cleaned definitions\n",
    "# Reduce the list of lists of words to a single list containing all words\n",
    "word_frequencies = get_words_frequency(ft.reduce(concat, clean_words))\n",
    "# Get the 15 most frequent words as the candidate genera\n",
    "candidates_genus = [wf[0] for wf in word_frequencies[:15]]\n",
    "\n",
    "# Initialize an empty list to store the senses\n",
    "senses = []\n",
    "\n",
    "# Use the modified Lesk algorithm to get a ranked list of probable senses for the candidate genera\n",
    "senses_rank = modifiedLesk(candidates_genus, disambiguation_context, stop_words)\n",
    "# Add the 5 most probable senses to the senses list\n",
    "top_senses = hq.nsmallest(5, senses_rank)\n",
    "\n",
    "print(\"Term we are looking for: door\\n\")\n",
    "print()\n",
    "for score, synset, definition in top_senses:\n",
    "      # Access the definition here\n",
    "    print(f\"Score: {abs(score)}\")\n",
    "    print(f\"Synset name: {synset.name()}\")\n",
    "    print(f\"Definition: {definition}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236392d2bb4b7e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ladybug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fc6690f2669e201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:55.035303740Z",
     "start_time": "2023-08-18T22:02:54.768381619Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term we are looking for: ladybug\n",
      "\n",
      "Score: 5\n",
      "Synset name: four-lined_plant_bug.n.01\n",
      "Definition: yellow or orange leaf bug with four black stripes down the back; widespread in central and eastern North America\n",
      "\n",
      "Score: 5\n",
      "Synset name: ladybug.n.01\n",
      "Definition: small round bright-colored and spotted beetle that usually feeds on aphids and other insect pests\n",
      "\n",
      "Score: 4\n",
      "Synset name: color.v.01\n",
      "Definition: add color to\n",
      "\n",
      "Score: 4\n",
      "Synset name: dipterous_insect.n.01\n",
      "Definition: insects having usually a single pair of functional wings (anterior pair) with the posterior pair reduced to small knobbed structures and mouth parts adapted for sucking or lapping or piercing\n",
      "\n",
      "Score: 4\n",
      "Synset name: lacewing.n.01\n",
      "Definition: any of two families of insects with gauzy wings (Chrysopidae and Hemerobiidae); larvae feed on insect pests such as aphids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_words = []\n",
    "for definition in ladybug_def:\n",
    "    clean_words = [clean_and_split(definition, stop_words) for definition in ladybug_def]\n",
    "disambiguation_context = ft.reduce(set.union, [set(words) for words in clean_words])\n",
    "\n",
    "\n",
    "\n",
    "word_frequencies = get_words_frequency(ft.reduce(concat, clean_words))\n",
    "candidates_genus = [wf[0] for wf in word_frequencies[:15]]\n",
    "\n",
    "\n",
    "\n",
    "senses = []\n",
    "senses_rank = modifiedLesk(candidates_genus, disambiguation_context, stop_words)\n",
    "senses.append(hq.nsmallest(5, senses_rank))\n",
    "\n",
    "# Add the 5 most probable senses to the senses list\n",
    "top_senses = hq.nsmallest(5, senses_rank)\n",
    "\n",
    "print(\"Term we are looking for: ladybug\")\n",
    "print()\n",
    "\n",
    "for score, synset, definition in top_senses:\n",
    "    \n",
    "    print(f\"Score: {abs(score)}\")\n",
    "    print(f\"Synset name: {synset.name()}\")\n",
    "    print(f\"Definition: {definition}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1f951a824734f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa45dc71455be4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:03:18.471307941Z",
     "start_time": "2023-08-18T22:03:17.701390707Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term we are looking for: pain\n",
      "\n",
      "Score: 7\n",
      "Synset name: bad.s.03\n",
      "Definition: feeling physical discomfort or pain (`tough' is occasionally used colloquially for `bad')\n",
      "\n",
      "Score: 5\n",
      "Synset name: affection.n.01\n",
      "Definition: a positive feeling of liking\n",
      "\n",
      "Score: 5\n",
      "Synset name: agony.n.01\n",
      "Definition: intense feelings of suffering; acute mental or physical pain\n",
      "\n",
      "Score: 5\n",
      "Synset name: ardor.n.01\n",
      "Definition: a feeling of strong eagerness (usually in favor of a person or cause)\n",
      "\n",
      "Score: 5\n",
      "Synset name: constriction.n.03\n",
      "Definition: a tight feeling in some part of the body\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_words = []\n",
    "for definition in pain_def:\n",
    "    clean_words = [clean_and_split(definition, stop_words) for definition in pain_def]\n",
    "disambiguation_context = ft.reduce(set.union, [set(words) for words in clean_words])\n",
    "\n",
    "\n",
    "word_frequencies = get_words_frequency(ft.reduce(concat, clean_words))\n",
    "candidates_genus = [wf[0] for wf in word_frequencies[:15]]\n",
    "\n",
    "\n",
    "senses = []\n",
    "senses_rank = modifiedLesk(candidates_genus, disambiguation_context, stop_words)\n",
    "senses.append(hq.nsmallest(5, senses_rank))\n",
    "\n",
    "# Add the 5 most probable senses to the senses list\n",
    "top_senses = hq.nsmallest(5, senses_rank)\n",
    "\n",
    "print(\"Term we are looking for: pain\")\n",
    "print()\n",
    "\n",
    "for score, synset, definition in top_senses:\n",
    "    \n",
    "    print(f\"Score: {abs(score)}\")\n",
    "    print(f\"Synset name: {synset.name()}\")\n",
    "    print(f\"Definition: {definition}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb43621a4de88b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Blurriness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60950d0dba1a2a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-18T22:02:58.073469140Z",
     "start_time": "2023-08-18T22:02:55.221950805Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term we are looking for: blurriness\n",
      "\n",
      "Score: 6\n",
      "Synset name: accommodating_lens_implant.n.01\n",
      "Definition: a lens implant containing a hinge that allows for both near and far vision (thus mimicking the natural lens of a young person)\n",
      "\n",
      "Score: 6\n",
      "Synset name: collage.n.01\n",
      "Definition: a paste-up made by sticking together pieces of paper or photographs to form an artistic image\n",
      "\n",
      "Score: 6\n",
      "Synset name: picture.n.01\n",
      "Definition: a visual representation (of an object or scene or person or abstraction) produced on a surface\n",
      "\n",
      "Score: 5\n",
      "Synset name: acuity.n.01\n",
      "Definition: sharpness of vision; the visual ability to resolve fine detail (usually measured by a Snellen chart)\n",
      "\n",
      "Score: 5\n",
      "Synset name: adapter.n.02\n",
      "Definition: device that enables something to be used in a way different from that for which it was intended or makes different pieces of apparatus compatible\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_words = []\n",
    "for definition in blurriness_def:\n",
    "    clean_words = [clean_and_split(definition, stop_words) for definition in blurriness_def]\n",
    "disambiguation_context = ft.reduce(set.union, [set(words) for words in clean_words])\n",
    "\n",
    "\n",
    "word_frequencies = get_words_frequency(ft.reduce(concat, clean_words))\n",
    "candidates_genus = [wf[0] for wf in word_frequencies[:15]]\n",
    "\n",
    "\n",
    "senses = []\n",
    "senses_rank = modifiedLesk(candidates_genus, disambiguation_context, stop_words)\n",
    "senses.append(hq.nsmallest(5, senses_rank))\n",
    "# Add the 5 most probable senses to the senses list\n",
    "top_senses = hq.nsmallest(5, senses_rank)\n",
    "\n",
    "print(\"Term we are looking for: blurriness\")\n",
    "print()\n",
    "\n",
    "for score, synset, definition in top_senses:\n",
    "    \n",
    "    print(f\"Score: {abs(score)}\")\n",
    "    print(f\"Synset name: {synset.name()}\")\n",
    "    print(f\"Definition: {definition}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
