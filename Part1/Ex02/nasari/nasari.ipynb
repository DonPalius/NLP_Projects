{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization Algorithm\n",
    "\n",
    "This algorithm aims to reduce the document size by a certain percentage (e.g., 10%, 20%, 30%) through an extractive approach.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Topic Identification:**\n",
    "   Identify the topic of the text to be summarized. The topic can be represented as a set of NASARI vectors:\n",
    "   \n",
    "\n",
    "2. **Context Creation:**\n",
    "Create the context by collecting the vectors of terms. This step can be repeated, gradually incorporating the contribution of associated terms at each round.\n",
    "\n",
    "3. **Sentence Retention:**\n",
    "Retain paragraphs whose sentences contain the most salient terms. Use the Weighted Overlap (WO) metric to determine the salience of terms in paragraphs. The WO metric is calculated as WO(v1, v2).\n",
    "\n",
    "4. **Paragraph Reranking:**\n",
    "Re-rank the retained paragraphs' weight by applying at least one of the following approaches:\n",
    "- Title-based approach\n",
    "- Cue-based approach\n",
    "- Phrase-based approach\n",
    "- Cohesion-based approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import namedtuple\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "compression_rate = [10, 20, 30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization Functions Documentation\n",
    "\n",
    "This documentation provides an overview and explanation of the various functions used in the extractive summarization process.\n",
    "\n",
    "## 1. `create_nasari_dict()`\n",
    "\n",
    "This function creates a Nasari dictionary from a given file containing NASARI vectors. It reads the NASARI file, processes each row to extract the key and corresponding vector values, and constructs a dictionary mapping keys (words) to Nasari vectors.\n",
    "\n",
    "## 2. `read_document(document)`\n",
    "\n",
    "This function reads a document file and extracts paragraphs from it. It skips comment lines and short paragraphs to assemble a list of extracted paragraphs, which will be used for further processing.\n",
    "\n",
    "## 3. `get_topic_words(paragraphs)`\n",
    "\n",
    "This function extracts topic words from the title and selected paragraphs. It combines words from the specified indices (title, first paragraph, last paragraph) to form a set of topic words that capture the main theme of the document.\n",
    "\n",
    "## 4. `get_context(topic_words, nasari_dict)`\n",
    "\n",
    "This function generates a Nasari context for a given set of topic words. It retrieves the Nasari vectors corresponding to the topic words from the Nasari dictionary and constructs a set of Nasari vectors representing the context.\n",
    "\n",
    "## 5. `rank(word, vector)`\n",
    "\n",
    "This function ranks a word within a Nasari vector. Given a word and a Nasari vector, it returns the rank of the word in the vector. The rank indicates the position of the word in the vector.\n",
    "\n",
    "## 6. `weighted_overlap(v1, v2)`\n",
    "\n",
    "This function calculates the weighted overlap between two Nasari vectors. It takes two Nasari vectors and computes their weighted overlap score, which indicates the similarity between the vectors based on the ranks of overlapping words.\n",
    "\n",
    "## 7. `rank_paragraphs(paragraphs, context, dict_nasari)`\n",
    "\n",
    "This function ranks paragraphs based on their similarity to a given context represented by Nasari vectors. It computes a rank score for each paragraph by calculating the weighted overlap between the paragraph and the context. Paragraphs are then ranked based on their rank scores.\n",
    "\n",
    "## 8. `summarize(document, ranks, title, paragraphs, c_rate)`\n",
    "\n",
    "This function generates a summary for a document based on the provided ranks and compression rate. It writes the summary to a file, considering the compression rate and the accumulated word count of the selected paragraphs. Additionally, it calculates BLEU and ROUGE metrics to evaluate the quality of the generated summary.\n",
    "\n",
    "## 9. `bleu_rouge(retrieved_document, c_rate, document)`\n",
    "\n",
    "This function calculates BLEU and ROUGE metrics to assess the quality of the generated summary. It compares the set of retrieved words in the summary with the set of relevant words from the original document to calculate precision and recall scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a Nasari dictionary from a given file\n",
    "def create_nasari_dict():\n",
    "    nasari_dict = {}\n",
    "    nasari_file_path = \"./dd-small-nasari-15.txt\"\n",
    "    \n",
    "    with open(nasari_file_path, encoding=\"utf8\") as file:\n",
    "        for row in file:\n",
    "            row_parts = row.split(\";\")\n",
    "            key = row_parts[1].lower()\n",
    "            \n",
    "            if len(row_parts) == 17:\n",
    "                if key in nasari_dict:\n",
    "                    key = row_parts[2].lower().split(\", \")[0]\n",
    "                value = row_parts[3:16] + [row_parts[16].strip('\\n')]\n",
    "            else:\n",
    "                value = row_parts[2:15] + [row_parts[15].strip('\\n')]\n",
    "            \n",
    "            nasari_dict[key] = value\n",
    "    \n",
    "    return nasari_dict\n",
    "\n",
    "# Function to read and extract paragraphs from a document file\n",
    "def read_document(document):\n",
    "    \"\"\"\n",
    "    Read and extract paragraphs from a document file.\n",
    "\n",
    "    Args:\n",
    "        document: The name of the document file.\n",
    "\n",
    "    Returns:\n",
    "        The list of extracted paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = []\n",
    "    with open('../obj/{}'.format(document), encoding='utf-8') as file:\n",
    "        for paragraph in file:\n",
    "            if not paragraph.startswith('#') and len(paragraph) > 1:\n",
    "                paragraphs.append(paragraph)\n",
    "    return paragraphs\n",
    "\n",
    "# Function to extract the topic words from title and paragraphs\n",
    "def get_topic_words(paragraphs):\n",
    "    \"\"\"\n",
    "    Extract the topic words from the title and paragraphs.\n",
    "\n",
    "    Args:\n",
    "        paragraphs: The list of paragraphs.\n",
    "\n",
    "    Returns:\n",
    "        The set of topic words.\n",
    "    \"\"\"\n",
    "# Function to extract the topic words from title and paragraphs\n",
    "    topic_words = set()\n",
    "    indices = [0, 1, -1]\n",
    "    \n",
    "    for index in indices:\n",
    "        paragraph = paragraphs[index].lower().translate(str.maketrans('', '', ''.join(PUNCTUATION)))\n",
    "        words = [word for word in paragraph.split() if word.isalpha() and word not in stop_words]\n",
    "        topic_words.update(words)\n",
    "    \n",
    "    return topic_words\n",
    "\n",
    "# Function to get the Nasari context for a given topic\n",
    "def get_context(topic_words, nasari_dict):\n",
    "    \"\"\"\n",
    "    Get the Nasari context for a given topic.\n",
    "\n",
    "    Args:\n",
    "        topic_words: The set of topic words.\n",
    "        nasari_dict: The Nasari dictionary.\n",
    "\n",
    "    Returns:\n",
    "        The set of Nasari vectors representing the context.\n",
    "    \"\"\"\n",
    "    context = set()\n",
    "    for word in topic_words:\n",
    "        if word in nasari_dict:\n",
    "            context.update(nasari_dict[word])\n",
    "    return context\n",
    "\n",
    "\n",
    "# Function to rank a word within a Nasari vector\n",
    "def rank(word, vector):\n",
    "    \"\"\"\n",
    "    Rank a word within a Nasari vector.\n",
    "\n",
    "    Args:\n",
    "        word: The word to rank.\n",
    "        vector: The Nasari vector.\n",
    "\n",
    "    Returns:\n",
    "        The rank of the word in the vector.\n",
    "    \"\"\"\n",
    "    return vector.index(word) + 1\n",
    "\n",
    "# Function to calculate weighted overlap between two Nasari vectors\n",
    "def weighted_overlap(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculates the weighted overlap between two Nasari vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    v1 (list): The first Nasari vector.\n",
    "    v2 (list): The second Nasari vector.\n",
    "    \n",
    "    Returns:\n",
    "    float: The weighted overlap score between v1 and v2.\n",
    "    \"\"\"\n",
    "    numerator, denominator = 0, 0\n",
    "    i = 1\n",
    "    overlap = set(v1).intersection(set(v2))\n",
    "    \n",
    "    for word in overlap:\n",
    "        numerator += pow(rank(word, v1) + rank(word, v2), -1)\n",
    "        denominator += pow(2 * i, -1)\n",
    "        i += 1\n",
    "        \n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return numerator / denominator\n",
    "\n",
    "# Function to rank paragraphs based on their similarity to a context\n",
    "def rank_paragraphs(paragraphs, context,dict_nasari):\n",
    "    \"\"\"\n",
    "    Ranks paragraphs based on their similarity to a given context.\n",
    "    \n",
    "    Parameters:\n",
    "    paragraphs (list): List of paragraphs to be ranked.\n",
    "    context (set): Set of Nasari vectors representing the context.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of named tuples containing index, rank score, and paragraph.\n",
    "    \"\"\"\n",
    "    ranks = []\n",
    "    RankedParagraph = namedtuple('RankedParagraph', ['index', 'rank_score', 'paragraph'])\n",
    "    \n",
    "    for index, paragraph in enumerate(paragraphs):  # Exclude the title\n",
    "        rank_score = 0\n",
    "        \n",
    "        # Tokenize and preprocess the paragraph\n",
    "        tokenized_paragraph = set(paragraph.lower().translate(str.maketrans('', '', ''.join(PUNCTUATION))).split()) - stop_words\n",
    "        \n",
    "        for pair in product(set([word for word in tokenized_paragraph]), context):\n",
    "            if pair[0] in dict_nasari and len(pair[1]) > 0 and pair[1][0] in dict_nasari:\n",
    "                # Calculate the weighted overlap score and accumulate rank score\n",
    "                rank_score += weighted_overlap(dict_nasari[pair[0]], dict_nasari[pair[1][0]])\n",
    "        \n",
    "        # Store the ranked paragraph information\n",
    "        ranks.append(RankedParagraph(index, rank_score, paragraph))\n",
    "    \n",
    "    # Return the ranked paragraphs sorted by rank score in descending order\n",
    "    return sorted(ranks, key=lambda rp: rp.rank_score, reverse=True)\n",
    "\n",
    "\n",
    "# ... (existing code)\n",
    "\n",
    "# Function to generate a summary for a document\n",
    "def summarize(document, ranks, title, paragraphs, c_rate):\n",
    "    \"\"\"\n",
    "    Generate a summary for a document.\n",
    "\n",
    "    Args:\n",
    "        document: The name of the document.\n",
    "        ranks: The list of ranked paragraphs.\n",
    "        title: The title of the document.\n",
    "        paragraphs: The list of paragraphs.\n",
    "        c_rate: The compression rate.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"    \n",
    "    file = open('../summaries/{}_{}'.format(c_rate, document), 'w+')\n",
    "    retrieved_document = set()\n",
    "    file.write(title + '\\n')\n",
    "    document_words = len(\" \".join(paragraphs).split())  # Calculate total words in the document\n",
    "    max_words = int((document_words * (100 - c_rate)) / 100)\n",
    "    words = 0\n",
    "    \n",
    "    for elem in ranks:\n",
    "        words += len(paragraphs[elem[0]].split())\n",
    "        \n",
    "        if words <= max_words:\n",
    "            file.write(paragraphs[elem[0]])\n",
    "            for word in paragraphs[elem[0]].translate(str.maketrans('', '', ''.join(PUNCTUATION))).split():\n",
    "                retrieved_document.add(word)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    file.close()\n",
    "    bleu_rouge(retrieved_document, c_rate, document)\n",
    "\n",
    "# Function to calculate BLEU and ROUGE metrics\n",
    "def bleu_rouge(retrieved_document, c_rate, document):\n",
    "    \"\"\"\n",
    "    Calculate BLEU and ROUGE metrics.\n",
    "\n",
    "    Args:\n",
    "        retrieved_document: The set of retrieved words in the document.\n",
    "        c_rate: The compression rate.\n",
    "        document: The name of the document.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"    \n",
    "    file =  open('../target/{}_{}'.format(c_rate, document))\n",
    "    paragraphs = list()\n",
    "    relevant_document = set()\n",
    "    \n",
    "    for paragraph in file:\n",
    "        if not paragraph.startswith('#') and len(paragraph) > 1:\n",
    "            paragraphs.append(paragraph)\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        for word in paragraph.translate(str.maketrans('', '', ''.join(PUNCTUATION))).split():\n",
    "            relevant_document.add(word)\n",
    "    \n",
    "    original_document_len = len(relevant_document)\n",
    "    summarized_document_len = len(retrieved_document)\n",
    "    \n",
    "    bleu_precision = len(relevant_document.intersection(retrieved_document)) / len(retrieved_document)\n",
    "    print('\\nDocument', document, ' Compression rate', c_rate)\n",
    "    print('BLEU Precision:', bleu_precision)\n",
    "    \n",
    "    rouge_recall = len(relevant_document.intersection(retrieved_document)) / len(relevant_document)\n",
    "    print('ROUGE Recall:', rouge_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarization Process Overview\n",
    "\n",
    "This overview outlines the steps involved in the extractive summarization process using the provided code block. The process involves extracting key information from a collection of documents and generating concise summaries based on identified context and topic words.\n",
    "\n",
    "## Initialization and Setup\n",
    "\n",
    "- The process begins with the setup, where required libraries and modules are imported.\n",
    "- A list of English stop words is prepared to aid in text processing.\n",
    "- A Nasari dictionary is created to store word-to-Nasari-vector mappings.\n",
    "\n",
    "## Document Summarization Loop\n",
    "\n",
    "- A loop iterates through a set of documents in a designated directory, aiming to generate summaries for each document.\n",
    "\n",
    "### Key Steps in the Loop:\n",
    "\n",
    "1. **Document Extraction:**\n",
    "   - Document contents are extracted into paragraphs using the `read_document()` function.\n",
    "\n",
    "2. **Topic Identification:**\n",
    "   - Topic words are extracted from the title and specific paragraphs using the `get_topic_words()` function. These words help capture the document's main theme.\n",
    "\n",
    "3. **Title Extraction:**\n",
    "   - The title of the document is extracted and removed from the list of paragraphs.\n",
    "\n",
    "4. **Context Generation:**\n",
    "   - A context is generated using the topic words and Nasari vectors through the `get_context()` function.\n",
    "\n",
    "5. **Paragraph Ranking:**\n",
    "   - The remaining paragraphs are ranked based on their similarity to the generated context using the `rank_paragraphs()` function.\n",
    "\n",
    "6. **Summarization at Different Compression Rates:**\n",
    "   - The script iterates over various compression rates, and for each rate, a summary is generated using the `summarize()` function. The generated summaries are saved for evaluation.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "- The generated summaries generated using https://resoomer.com/en are evaluated for quality using BLEU and ROUGE metrics. These metrics measure the precision and recall of the generated summaries compared to the original document's content.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Ebola-virus-disease.txt  Compression rate 10\n",
      "BLEU Precision: 0.9678714859437751\n",
      "ROUGE Recall: 0.9281129653401797\n",
      "\n",
      "Document Ebola-virus-disease.txt  Compression rate 20\n",
      "BLEU Precision: 0.8330871491875923\n",
      "ROUGE Recall: 0.8545454545454545\n",
      "\n",
      "Document Ebola-virus-disease.txt  Compression rate 30\n",
      "BLEU Precision: 0.729235880398671\n",
      "ROUGE Recall: 0.7453310696095077\n",
      "\n",
      "Document Napoleon-wiki.txt  Compression rate 10\n",
      "BLEU Precision: 0.9852941176470589\n",
      "ROUGE Recall: 0.8427672955974843\n",
      "\n",
      "Document Napoleon-wiki.txt  Compression rate 20\n",
      "BLEU Precision: 0.830238726790451\n",
      "ROUGE Recall: 0.839142091152815\n",
      "\n",
      "Document Napoleon-wiki.txt  Compression rate 30\n",
      "BLEU Precision: 0.7521865889212828\n",
      "ROUGE Recall: 0.7588235294117647\n",
      "\n",
      "Document Life-indoors.txt  Compression rate 10\n",
      "BLEU Precision: 0.9434628975265018\n",
      "ROUGE Recall: 0.8585209003215434\n",
      "\n",
      "Document Life-indoors.txt  Compression rate 20\n",
      "BLEU Precision: 0.9442231075697212\n",
      "ROUGE Recall: 0.855595667870036\n",
      "\n",
      "Document Life-indoors.txt  Compression rate 30\n",
      "BLEU Precision: 0.9381443298969072\n",
      "ROUGE Recall: 0.7913043478260869\n",
      "\n",
      "Document Trump-wall.txt  Compression rate 10\n",
      "BLEU Precision: 0.7734165923282783\n",
      "ROUGE Recall: 0.9475409836065574\n",
      "\n",
      "Document Trump-wall.txt  Compression rate 20\n",
      "BLEU Precision: 0.7090909090909091\n",
      "ROUGE Recall: 0.9439490445859873\n",
      "\n",
      "Document Trump-wall.txt  Compression rate 30\n",
      "BLEU Precision: 0.6921465968586388\n",
      "ROUGE Recall: 0.9456366237482118\n",
      "\n",
      "Document Andy-Warhol.txt  Compression rate 10\n",
      "BLEU Precision: 0.8324420677361853\n",
      "ROUGE Recall: 0.8354203935599285\n",
      "\n",
      "Document Andy-Warhol.txt  Compression rate 20\n",
      "BLEU Precision: 0.8523622047244095\n",
      "ROUGE Recall: 0.9232409381663113\n",
      "\n",
      "Document Andy-Warhol.txt  Compression rate 30\n",
      "BLEU Precision: 0.8515463917525773\n",
      "ROUGE Recall: 0.9649532710280374\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "nasari_dict = create_nasari_dict()\n",
    "os.chdir(\"./obj/\")\n",
    "for root, _, files in os.walk(os.getcwd()):\n",
    "    list_files = files\n",
    "\n",
    "for document in list_files:\n",
    "    list_paragraphs = read_document(document)\n",
    "    topic_words = get_topic_words(list_paragraphs)\n",
    "    \n",
    "    title = list_paragraphs[0]\n",
    "    list_paragraphs.pop(0)\n",
    "    context = get_context(topic_words, nasari_dict)\n",
    "    ranks = rank_paragraphs(list_paragraphs, context,nasari_dict)\n",
    "    for rate in compression_rate: \n",
    "        summarize(document, ranks, title, list_paragraphs, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Results and Potential Enhancements\n",
    "\n",
    "The provided results showcase the performance of the extractive summarization process across various documents and compression rates. The evaluation is conducted using BLEU and ROUGE metrics to measure the precision and recall of the generated summaries compared to the original document content.\n",
    "\n",
    "## Observations:\n",
    "\n",
    "- The BLEU precision scores indicate the level of overlap between the generated summary and the original document, where higher scores signify greater overlap.\n",
    "- The ROUGE recall scores reflect the ability of the summary to capture important information present in the document, with higher scores indicating better recall.\n",
    "\n",
    "## Document-Specific Insights:\n",
    "\n",
    "- For the document \"Ebola-virus-disease.txt,\" at a compression rate of 10%, the generated summary achieves high precision and recall, indicating a well-balanced and informative summary.\n",
    "- The document \"Napoleon-wiki.txt\" demonstrates consistently high BLEU precision scores across different compression rates, suggesting that the summary effectively captures the core content.\n",
    "- The document \"Life-indoors.txt\" showcases notable precision at 10% compression, while there is a decrease in recall as the compression rate increases to 30%.\n",
    "- In the case of \"Trump-wall.txt,\" higher compression rates lead to a slight decrease in both BLEU precision and ROUGE recall scores, indicating potential loss of context.\n",
    "- The document \"Andy-Warhol.txt\" demonstrates a favorable balance between BLEU precision and ROUGE recall, with improved recall at higher compression rates.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
