{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Generating Synthetic Trump-Like Tweets\n",
    "\n",
    "In this task, you are tasked with creating a language model based on n-grams from approximately 300 tweets extracted from Donald Trump's Twitter profile. You will then generate synthetic tweets that emulate the style of Trump's tweets using this language model.\n",
    "\n",
    "### Requirements:\n",
    "- Build an n-gram-based language model using the provided tweet dataset.\n",
    "- Generate synthetic Trump-like tweets using the trained language model.\n",
    "- Evaluate the perplexity of the language model on the generated tweets.\n",
    "\n",
    "### Problem\n",
    "The problem at hand involves developing a language model that can mimic the writing style of Donald Trump in his tweets. To achieve this, we will train an n-gram-based language model using a dataset containing around 300 of Trump's tweets and then use this model to generate synthetic tweets in his distinctive style.\n",
    "\n",
    "### Solution\n",
    "The solution involves the following steps:\n",
    "\n",
    "#### Data Preprocessing\n",
    "1. Load the dataset containing Trump's tweets from a CSV file.\n",
    "2. Clean and preprocess each tweet by removing special characters, URLs, and normalizing certain punctuation marks. Tokenize the cleaned tweets using the TweetTokenizer from NLTK.\n",
    "3. Calculate basic statistics on the dataset, including the average length in characters and words of the tweets.\n",
    "4. Split in train and test sets\n",
    "\n",
    "#### Language Model Training\n",
    "4. Choose the value of 'N' for the n-grams (e.g., bigram or trigram).\n",
    "5. Create n-grams and a vocabulary from the preprocessed tweet corpus using the NLTK library.\n",
    "6. Train a Laplace-smoothed n-gram language model on the n-grams and vocabulary.\n",
    "\n",
    "#### Synthetic Tweet Generation\n",
    "7. Generate synthetic Trump-like tweets using the trained language model.\n",
    "8. For each generated tweet, ensure that it meets a specified word count and does not contain duplicate sentences.\n",
    "\n",
    "#### Perplexity Evaluation\n",
    "9. Calculate the perplexity of the trained language model on the test set. Perplexity measures how well the model predicts the generated text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Esercitazione 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tweet like (a) Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T17:04:35.346977Z",
     "start_time": "2023-05-19T17:04:34.520725Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize, TreebankWordDetokenizer\n",
    "from nltk.lm import Laplace, MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from itertools import chain\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T17:04:35.358570Z",
     "start_time": "2023-05-19T17:04:35.347889Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>LOSER! https://t.co/p5imhMJqS1</td>\n",
       "      <td>05-18-2020 14:55:14</td>\n",
       "      <td>32295</td>\n",
       "      <td>135445</td>\n",
       "      <td>False</td>\n",
       "      <td>1262396333064892416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Most of the money raised by the RINO losers of...</td>\n",
       "      <td>05-05-2020 18:18:26</td>\n",
       "      <td>19706</td>\n",
       "      <td>82425</td>\n",
       "      <td>False</td>\n",
       "      <td>1257736426206031874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>....because they don’t know how to win and the...</td>\n",
       "      <td>05-05-2020 04:46:34</td>\n",
       "      <td>12665</td>\n",
       "      <td>56868</td>\n",
       "      <td>False</td>\n",
       "      <td>1257532112233803782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>....lost for Evan “McMuffin” McMullin (to me)....</td>\n",
       "      <td>05-05-2020 04:46:34</td>\n",
       "      <td>13855</td>\n",
       "      <td>62268</td>\n",
       "      <td>False</td>\n",
       "      <td>1257532114666508291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>....get even for all of their many failures. Y...</td>\n",
       "      <td>05-05-2020 04:46:33</td>\n",
       "      <td>8122</td>\n",
       "      <td>33261</td>\n",
       "      <td>False</td>\n",
       "      <td>1257532110971318274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>.@Cher attacked @MittRomney. She is an average...</td>\n",
       "      <td>05-10-2012 15:10:23</td>\n",
       "      <td>715</td>\n",
       "      <td>465</td>\n",
       "      <td>False</td>\n",
       "      <td>200603697435246592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>Firing @lisalampanelli may have come as a surp...</td>\n",
       "      <td>05-07-2012 02:58:18</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>199332301463748609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>My @SquawkCNBC interview discussing why @MittR...</td>\n",
       "      <td>03-06-2012 17:07:51</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>177078050750599168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>I feel sorry for Rosie 's new partner in love ...</td>\n",
       "      <td>12-14-2011 16:45:55</td>\n",
       "      <td>667</td>\n",
       "      <td>463</td>\n",
       "      <td>False</td>\n",
       "      <td>146994336670822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>TweetDeck</td>\n",
       "      <td>The S&amp;P are losers. They did this for personal...</td>\n",
       "      <td>08-09-2011 17:40:33</td>\n",
       "      <td>209</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>100984825103663104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source                                               text  \\\n",
       "0    Twitter for iPhone                     LOSER! https://t.co/p5imhMJqS1   \n",
       "1    Twitter for iPhone  Most of the money raised by the RINO losers of...   \n",
       "2    Twitter for iPhone  ....because they don’t know how to win and the...   \n",
       "3    Twitter for iPhone  ....lost for Evan “McMuffin” McMullin (to me)....   \n",
       "4    Twitter for iPhone  ....get even for all of their many failures. Y...   \n",
       "..                  ...                                                ...   \n",
       "256  Twitter Web Client  .@Cher attacked @MittRomney. She is an average...   \n",
       "257  Twitter Web Client  Firing @lisalampanelli may have come as a surp...   \n",
       "258  Twitter Web Client  My @SquawkCNBC interview discussing why @MittR...   \n",
       "259           TweetDeck  I feel sorry for Rosie 's new partner in love ...   \n",
       "260           TweetDeck  The S&P are losers. They did this for personal...   \n",
       "\n",
       "              created_at  retweet_count  favorite_count is_retweet  \\\n",
       "0    05-18-2020 14:55:14          32295          135445      False   \n",
       "1    05-05-2020 18:18:26          19706           82425      False   \n",
       "2    05-05-2020 04:46:34          12665           56868      False   \n",
       "3    05-05-2020 04:46:34          13855           62268      False   \n",
       "4    05-05-2020 04:46:33           8122           33261      False   \n",
       "..                   ...            ...             ...        ...   \n",
       "256  05-10-2012 15:10:23            715             465      False   \n",
       "257  05-07-2012 02:58:18             45              19      False   \n",
       "258  03-06-2012 17:07:51             32               9      False   \n",
       "259  12-14-2011 16:45:55            667             463      False   \n",
       "260  08-09-2011 17:40:33            209              19      False   \n",
       "\n",
       "                  id_str  \n",
       "0    1262396333064892416  \n",
       "1    1257736426206031874  \n",
       "2    1257532112233803782  \n",
       "3    1257532114666508291  \n",
       "4    1257532110971318274  \n",
       "..                   ...  \n",
       "256   200603697435246592  \n",
       "257   199332301463748609  \n",
       "258   177078050750599168  \n",
       "259   146994336670822400  \n",
       "260   100984825103663104  \n",
       "\n",
       "[261 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../Trump/trump_twitter_archive/tweets.csv') # Caricamento del dataset\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    # Replace \"&amp\" with \"and\"\n",
    "    clean_tweet = re.sub(r\"&amp\", \"and\", tweet)\n",
    "    # Replace \"&\" with \"and\"\n",
    "    clean_tweet = re.sub(r\"&\", \"and\", clean_tweet)\n",
    "    # Remove multiple consecutive dots (ellipsis)\n",
    "    clean_tweet = re.sub(r'^\\.{2,}', '', clean_tweet)\n",
    "    # Replace curly single quotes with straight single quotes\n",
    "    clean_tweet = re.sub(r\"’\", \"'\", clean_tweet)\n",
    "    # Replace semicolons with single quotes\n",
    "    clean_tweet = re.sub(r';', \"'\", clean_tweet)\n",
    "    # Remove double quotation marks\n",
    "    clean_tweet = re.sub(r'“', '', clean_tweet)\n",
    "    clean_tweet = re.sub(r'\"', '', clean_tweet)\n",
    "    # Replace double hyphens with a space\n",
    "    clean_tweet = re.sub(r'--', ' ', clean_tweet)\n",
    "    # Remove text within parentheses, e.g., (text)\n",
    "    clean_tweet = re.sub(r'\\([^)]*\\)', '', clean_tweet)\n",
    "\n",
    "    # Define patterns to identify special tokens\n",
    "    usernamepattern = r'@[a-zA-Z0-9]+'\n",
    "    hashtagpattern = r'#[a-zA-Z0-9]+'\n",
    "    websitepattern = r'http\\S+|www\\S+|https\\S+'\n",
    "\n",
    "    # Initialize a TweetTokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    # Tokenize the cleaned tweet\n",
    "    tokens = tokenizer.tokenize(clean_tweet)\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    # Iterate through the tokens\n",
    "    for token in tokens:\n",
    "        # Check if the token matches one of the special token patterns\n",
    "        if re.match(usernamepattern, token) or re.match(hashtagpattern, token) or re.match(websitepattern, token):\n",
    "            cleaned_tokens.extend([token])  # Add the special token as a whole\n",
    "        else:\n",
    "            cleaned_token = token  # Keep punctuation and regular words\n",
    "            # Check if the token is not an empty string\n",
    "            if token.strip():\n",
    "                cleaned_tokens.append(cleaned_token)\n",
    "\n",
    "    # Return the list of cleaned and tokenized tokens\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT IN TRAIN AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 208\n",
      "Testing set size: 53\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "train_corpus, test_corpus = train_test_split(list(df['text']), test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Verify the sizes of the training and testing sets\n",
    "print(\"Training set size:\", len(train_corpus))\n",
    "print(\"Testing set size:\", len(test_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_corpus = []\n",
    "trump_test_corpus = []\n",
    "for tweet in train_corpus:\n",
    "    trump_corpus.append(tweet_cleaner(tweet))\n",
    "\n",
    "for tweet in test_corpus:\n",
    "    trump_test_corpus.append(tweet_cleaner(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average word per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 26)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len, avg_word = sum([len(t) for t in df['text']]) // len(df), sum(len(t.split()) for t in df['text']) // len(df)\n",
    "avg_len, avg_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND TRAIN THE MODEL bi gram and tri gram laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bigram or trigram\n",
    "# N = 2\n",
    "N = 2\n",
    "\n",
    "n_grams, vocabulary = padded_everygram_pipeline(N, trump_corpus)\n",
    "trump_model = Laplace(N)\n",
    "trump_model.fit(n_grams, vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "tri_grams, vocabulary_3 = padded_everygram_pipeline(N, trump_corpus)\n",
    "trump_model_3 = Laplace(3)\n",
    "trump_model_3.fit(tri_grams, vocabulary_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE AND TRAIN THE MODEL tri gram MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_mle, vocabulary_mle_2 = padded_everygram_pipeline(2, trump_corpus)\n",
    "trump_model_MLE_2 = MLE(2)\n",
    "trump_model_MLE_2.fit(bi_gram_mle, vocabulary_mle_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tri_gram_mle, vocabulary_mle_3 = padded_everygram_pipeline(3, trump_corpus)\n",
    "trump_model_MLE_3 = MLE(3)\n",
    "trump_model_MLE_3.fit(tri_gram_mle, vocabulary_mle_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GENERATE THE SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sent(model, random_seed, num_words):\n",
    "    actual_count = 0  # Initialize the count of generated words\n",
    "    synthetic_sentence = None  # Initialize the synthetic sentence\n",
    "    attempts = 0  # Initialize the number of attempts\n",
    "    isFinish = False  # Flag to indicate if generation is finished\n",
    "    generated_sentences = set()  # Initialize a set to track generated sentences and avoid duplicates\n",
    "    \n",
    "    while actual_count < num_words and not isFinish:\n",
    "        # Generate a sequence of words using the provided model and random seed\n",
    "        generated_words = model.generate(num_words, random_seed=random_seed)\n",
    "        content = []  # Initialize a list to store the content of the synthetic sentence\n",
    "        \n",
    "        for word, _ in itertools.groupby(generated_words):\n",
    "            if word.strip():\n",
    "                if word == '<s>':\n",
    "                    continue\n",
    "                if word == '</s>':\n",
    "                    if actual_count < num_words:\n",
    "                        continue\n",
    "\n",
    "                if len(content) == num_words:\n",
    "                    if word == '</s>':\n",
    "                        \n",
    "                        isFinish = True  # Finish generation if we have enough words\n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "                if word != '<s>' and word != '</s>':\n",
    "                    content.append(word)\n",
    "                    actual_count = len(content)  # Update the word count\n",
    "        \n",
    "        random_seed = random.randint(1, 100)  # Update random_seed for the next attempt\n",
    "        attempts += 1\n",
    "        \n",
    "    # Detokenize the content to form a synthetic sentence\n",
    "    synthetic_sentence = TreebankWordDetokenizer().detokenize(content)\n",
    "    \n",
    "    # Check if the synthetic sentence is not already generated to avoid duplicates\n",
    "    if synthetic_sentence not in generated_sentences:\n",
    "        generated_sentences.add(synthetic_sentence)\n",
    "    else:\n",
    "        return None, None  # Return None if the sentence is a duplicate\n",
    "    \n",
    "    # Return the synthetic sentence and its content\n",
    "    return synthetic_sentence, content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAPLACE bigram and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI GRAM\n",
      "Real Sentence: which I LOVED the Philippines I only horse in a wig realize to win one thing he couldn't get out nasty guy who is the other\n",
      "Real Sentence: know I never say these lowlifes follow-nothing to look a real positive in conjunction with in this special date September 11th . EVERYONE knows that.Some LOSERS\n",
      "Real Sentence: to him is a disgrace to all about Andrew McCabe Peter S and continue to run for iPhone, false reporting . But I don't like\n",
      "Real Sentence: don't even the act . Do Nothing Dems are jealous and caught in anyway? Ignorance Yes he got out of the haters and losers a\n",
      "Real Sentence: @CNN which is so right from the Central Park 5 Twitter Web Client, Just ck how are jealous and the Tea Party . Stop busting\n",
      "Real Sentence: Fake News Awards those who hates Michael Wolff is a total fool . Hence he is still follow loser! Strong fierce noble, false reporting\n",
      "Real Sentence: Major loser with you-and a very big ratings are sick and is how lucky they will flip if they ever . It will miss him off\n",
      "Real Sentence: in politics yet follow him on Twitter for that Forbes is dying @NYDailyNews but if...save your panel we need a winner . Almost every\n",
      "Real Sentence: Lisa Page and winners from whatever they are a good aspect of fate . The Pledge? His stupidity could be dealt with her — you\n",
      "Real Sentence: 09-21- 2013 01:15: RT @realDonaldTrump I have not wear a month for President.Wow I hope that she should be done on his supporters since they\n",
      "\n",
      "################################\n",
      "\n",
      "TRI GRAM\n",
      "Real Sentence: Senator ” My @SquawkCNBC interview discussing why @KarlRove if I don't even for all LOSERS but winners from a real loser who spent hundreds of Bernie\n",
      "Real Sentence: President of fate . I told me . It is Joy-Ann Reid? His stupidity could be if I NEVER went hostile . They're all fathers\n",
      "Real Sentence: Senator ” My @SquawkCNBC interview discussing why @KarlRove if I don't even for all LOSERS but winners from a real loser who spent hundreds of Bernie\n",
      "Real Sentence: haters or high opinion of millions of the same old path of the United States by everyone including Karl Rove gave us Barack Obama is SALT\n",
      "Real Sentence: A . Had a clown.Yes and ultimately NO talent loser and a fine person reacts to the playoffs what Kellyanne Conway often referred to try and\n",
      "Real Sentence: haters or high opinion of millions of the same old path of the United States by everyone including Karl Rove gave us Barack Obama is SALT\n",
      "Real Sentence: will miss him is pushing Republicans down for Kasich . No wonder Cuomo and Border Security . For Growth tried to as a much is now\n",
      "Real Sentence: don't even the act . Do Nothing Dems are jealous and caught in anyway? Ignorance Yes he got out of the haters and losers a\n",
      "Real Sentence: and too . Begged my past-unlike our great nominee gas prices and' most others I wonder Cuomo and' would constantly miss meetings and'\n",
      "Real Sentence: in politics yet follow him on Twitter for that Forbes is dying @NYDailyNews but if...save your panel we need a winner . Almost every\n"
     ]
    }
   ],
   "source": [
    "\n",
    "synthetic_tweets = []\n",
    "# Generate synthetic tweets using the trained model\n",
    "print(\"BI GRAM\")\n",
    "for i in range(0, 10):\n",
    "    random_seed = random.randint(1, 100)  # Generate a new random seed for each tweet\n",
    "    tmp_tweet = generate_sent(trump_model, random_seed, num_words=avg_word)\n",
    "    if tmp_tweet[1] is not None:\n",
    "        if len(tmp_tweet[1]) > 0 and tmp_tweet[0]!=None:\n",
    "            synthetic_tweets.append(tmp_tweet)\n",
    "\n",
    "\n",
    "for real_sentence, _ in synthetic_tweets:\n",
    "    print(\"Real Sentence:\", real_sentence)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"################################\")\n",
    "print()\n",
    "print(\"TRI GRAM\")\n",
    "synthetic_tweets = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    random_seed = random.randint(1, 100)  # Generate a new random seed for each tweet\n",
    "    tmp_tweet = generate_sent(trump_model_3, random_seed, num_words=avg_word)\n",
    "    if tmp_tweet[1] is not None:\n",
    "        if len(tmp_tweet[1]) > 0 and tmp_tweet[0]!=None:\n",
    "            synthetic_tweets.append(tmp_tweet)\n",
    "\n",
    "\n",
    "for real_sentence, _ in synthetic_tweets:\n",
    "    print(\"Real Sentence:\", real_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE THE PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for N = 2: 1968.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity for N = {N}: {trump_model.perplexity(trump_test_corpus):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI GRAM\n",
      "Real Sentence: discussing why you sit this election cycle out of July to all even the nomination to N . @realDonaldTrump The many people will miss him and\n",
      "Real Sentence: cycle out we followed their so-called Lincoln Project is a total loser? He used Sloppy Steve has ZERO about power than tearing them out nasty\n",
      "Real Sentence: and that's why George Will the Military Vets and even called my call with people losing their false, 05-08- 2013 01:15: Mary Kissel is\n",
      "Real Sentence: Editorial on how bad and haters and' Reed Galvin lost . Cruz is doing this really hard but I will self-destruct just started blocking out\n",
      "Real Sentence: They should not been really boring and watch @CNN bore their own account / lawyer who sadly plays right? His stupidity could have no buyers\n",
      "Real Sentence: . Low Ratings . Kelly came on right from whatever they are invited to sell the people losing their bad ., 3815633303 4924032 0 Twitter\n",
      "Real Sentence: Cuban?, 3262266462 4598630 4 Twitter Web Client, 03-09- 2016 05:40: First Bloomberg is no Country is laughing at Karl Rove losers-true!\n",
      "Real Sentence: . and' protecting our rapidly rebuilding Military at why their own account / lawyer who hates Michael a bully . She is so badly damaged\n",
      "Real Sentence: . The winners from whatever they know you and' Trade are just as George Conway by these awards is that they are all fathers even\n",
      "Real Sentence: , @realDonaldTrump @DannyZuker Danny is out of moderate Dems are invited to look like corrupt and she went around and haters and losers in many people\n",
      "\n",
      "################################\n",
      "\n",
      "TRI GRAM\n",
      "Real Sentence: , @realDonaldTrump: I would never recommend that anyone use her lawyer he is a great nominee gas prices and why George Will is a made\n",
      "Real Sentence: are either jealous or losers bother Donald one bit haha . Do they affect you in anyway? No just a loser terrorist.These are sick and\n",
      "Real Sentence: 2013 14:28: 50,762, 846, false, 6216964297 4932582 5 Twitter for Android, @SirHatchporch: Mary Kissel is an SNL character right?\n",
      "Real Sentence: . @FoxNews, 07-16- 2015 15:02: 34,537, 845, false, 3208525689 2016640 1 Twitter for Android, Thanks @JamersonHayes they are so low\n",
      "Real Sentence: !, 04-22- 2013 06:51: 08,41, 31, false, 4675507957 2318208 0 Twitter Web Client, As everybody knows but the haters and\n",
      "Real Sentence: Twitter for Android, My ties and shirts are doing very big numbers @Macy' s continued incompetence has led to more people losing their jobs\n",
      "Real Sentence: about power than she should not be perfect but it's mine ., 05-08- 2013 04:43: 28,86, 91, false, 6787976312 5091123 2\n",
      "Real Sentence: Kissel is an SNL character right? #MorningJoe She is a winner . EVERYONE knows that.Some LOSERS don't acknowledge it yet follow him? Because I\n",
      "Real Sentence: a loser who hates Michael a fine person with a wonderful family . Michael is a joke not very bright and totally overrated some losers and\n",
      "Real Sentence: taken large amounts of campaign money from a Crooked Hillary source when Clinton was under investigation by the RINO losers of the year ” and which\n"
     ]
    }
   ],
   "source": [
    "\n",
    "synthetic_tweets = []\n",
    "# Generate synthetic tweets using the trained model\n",
    "print(\"BI GRAM\")\n",
    "for i in range(0, 10):\n",
    "    random_seed = random.randint(1, 100)  # Generate a new random seed for each tweet\n",
    "    tmp_tweet = generate_sent(trump_model_MLE_2, random_seed, num_words=avg_word)\n",
    "    if tmp_tweet[1] is not None:\n",
    "        if len(tmp_tweet[1]) > 0 and tmp_tweet[0]!=None:\n",
    "            synthetic_tweets.append(tmp_tweet)\n",
    "\n",
    "\n",
    "for real_sentence, _ in synthetic_tweets:\n",
    "    print(\"Real Sentence:\", real_sentence)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"################################\")\n",
    "print()\n",
    "print(\"TRI GRAM\")\n",
    "synthetic_tweets = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    random_seed = random.randint(1, 100)  # Generate a new random seed for each tweet\n",
    "    tmp_tweet = generate_sent(trump_model_MLE_3, random_seed, num_words=avg_word)\n",
    "    if tmp_tweet[1] is not None:\n",
    "        if len(tmp_tweet[1]) > 0 and tmp_tweet[0]!=None:\n",
    "            synthetic_tweets.append(tmp_tweet)\n",
    "\n",
    "\n",
    "for real_sentence, _ in synthetic_tweets:\n",
    "    print(\"Real Sentence:\", real_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE THE PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for N = 2: inf\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity for N = {N}: {trump_model_MLE_2.perplexity(trump_test_corpus):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
